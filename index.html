<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="MeGaBoT: A Meta Quest Dataset for Gaze, Body, and Hand Interaction in MR">
  <meta name="keywords" content="MeGaBoT">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>MeGaBoT: A Meta Quest Dataset for Gaze, Body, and Hand Interaction in MR</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>

  <nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>
  </nav>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">MeGaBoT: A Meta Quest Dataset for Gaze, Body, and Hand Interaction
              in MR</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://www.iit.it/en-US/people-details/-/people/natnael-takele">Natnael Berhanu
                  Takele</a><sup>1,2</sup>,</span>
              <span class="author-block">
                <a href="https://www.iit.it/it/people-details/-/people/yonas-tefera">Yonas Theodros
                  Tefera</a><sup>2</sup>,</span>
              <span class="author-block">
                <a href="https://www.iit.it/people-details/-/people/donatien-delehelle">Donatien
                  Delehelle</a><sup>2</sup>,</span>
              <span class="author-block">
                <a href="https://www.iit.it/people-details/-/people/darwin-caldwell">Darwin
                  Caldwell</a><sup>2</sup></span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>University of Genova,</span>
              <span class="author-block"><sup>2</sup>Istituto Italiano di Tecnologia</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <!-- <span class="link-block">
                <a href="https://arxiv.org/abs/2410.05038"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
                <!-- Video Link. -->
                <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=NpEaa2P7qZI"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
                <!-- Code Link. -->
                <span class="link-block">
                  <a href="https://github.com/NatnaelB7/MeGaBoT_Dataset"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
              </div>

            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <!-- <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/teaser.mp4"
                type="video/mp4">
      </video> -->
        <h2 class="subtitle has-text-centered">
          <span class="dnerf">MeGaBoT:</span> a Meta Quest Dataset for Gaze, Body, and Hand Interaction in Mixed Reality
        </h2>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              This paper presents MeGaBoT, a dataset that provides tracked
              gaze, upper body movements, and hand interaction joints in
              mixed reality (MR) environments.
            </p>
            <p>
              While humans intuitively manipulate garments
              and other textile items swiftly and accurately, it is a significant
              challenge for robots. A factor crucial to human performance
              is the ability to imagine, a priori, the intended result of the
              manipulation intents and hence develop predictions on the garment pose.
              That ability allows us to plan from highly obstructed
              states, adapt our plans as we collect more information and react
              swiftly to unforeseen circumstances. Conversely, robots struggle
              to establish such intuitions and form tight links between plans
              and observations.
            </p>
            <p>
              The dataset is based on the Meta Quest Pro headset, offering an easy-to-use
              resource for researchers and developers working in MR and human-computer interaction (HCI).
              MeGaBoT focuses on collecting natural and precise interactions with virtual objects, with
              three core interaction types: pushing, pointing, and grasping. To ensure robustness
              and generalization, each interaction is performed across six distinct directions, reflecting
              a diverse range of movement trajectories relative to the user's body.
              This directional diversity provides critical insights into how users approach and engage
              with virtual objects from multiple angles. To precisely track contact points during
              interactions, 17 key contact points are defined for each direction and are labeled.
              These contact points serve as reference markers to accurately localize and quantify
              the joint-to-object contact points for each interaction type and direction.
            </p>
            <p>
              In addition to providing the dataset, this paper evaluates the quality and
              precision of the collected dataset in MR through a set of comprehensive
              evaluation metrics. These metrics assess critical aspects of interaction
              performance, including Trajectory Similarity, Joint Angle Orientation, and
              Joint-to-Contact Alignment. It also details the theoretical and implementation
              considerations for dataset collection, offering valuable insights for
              applications in VR and human-robot interaction.
            </p>
            <p>
              This resource aims to support the development and evaluation of MR-based
              interaction systems.
            </p>
          </div>
        </div>
      </div>
      <!--/ Abstract. -->

      <!-- Paper video. -->
      <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div> -->
      <!--/ Paper video. -->
    </div>
  </section>


  <section class="section">
    <div class="container is-max-desktop">

      <div class="columns has-text-centered">
        <div class="column">
          <div class="content">
            <h2 class="title is-4">Pushing Interaction</h2>
            <video id="dollyzoom" class="border-solid" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/pushing_ktk_video.webm" type="video/webm">
              Your browser doesn't seem to support the video tag.
            </video>
          </div>
        </div>
        <div class="column">
          <div class="content">
            <h2 class="title is-4">Pointing Interaction</h2>
            <video id="dollyzoom" class="border-solid" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/pointing_ktk_video.webm" type="video/webm">
              Your browser doesn't seem to support the video tag.
            </video>
          </div>

        </div>
        <div class="column">
          <div class="content">
            <h2 class="title is-4">Grasping Interaction</h2>
            <video id="dollyzoom" class="border-solid" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/grasping_ktk_video.webm" type="video/webm">
              Your browser doesn't seem to support the video tag.
            </video>
          </div>
        </div>
      </div>
      <!--/ Matting. -->

      <!-- Overview. -->
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Overview</h2>

          <div class="content has-text-justified">
            <p>Gaze, hand gestures, and full-body interaction are among the most essential and natural human
              communication and interaction modes. They often serve as powerful non-verbal cues, allowing us to perceive
              and interpret information even before it is verbally conveyed. These interaction methods are particularly
              critical in applications like Virtual Reality (VR), where creating an immersive experience demands a
              seamless and intuitive flow of information.
              MeGaBoT provides a comprehensive dataset that captures body pose and eye tracking for immersive Mixed
              Reality (MR)-based remote haptic teleoperation. The dataset is accompanied by methodologies for collecting
              and analyzing complex user interactions in MR environments, enabling advancements in VR, human-computer
              interaction, and human-robot collaboration.
            </p>
          </div>
          <div class="columns is-centered overview-image">
            <img src="./static/images/Body_Tracking_page-0001.jpg" class="overview-image" alt="Body Tracking Image" />
          </div>
          <div class="columns is-centered mt-2"><b>Human Body Pose and Joints</b></div>

          <h2 class="title is-3">Experiemental Setup</h2>
          <div class="content has-text-justified">
            <p>
              The system is built on the Meta Quest Pro VR headset, which supports gaze, hand, body tracking, and motion
              controllers. The VR interface, developed using Unreal Engine 5 (UE-5), includes 3D interaction elements.
              Participants interact with a virtual plane containing specific contact points that anchor 3D mesh objects
              tailored to each task.
              Spherical meshes are used for pointing tasks, offering intuitive and precise targets. Rectangular meshes,
              with their larger surface areas, are designed for pushing tasks, allowing natural application of force.
              Handle-shaped meshes, resembling door handles, are employed for grasping tasks, promoting realistic grip
              and hand poses.
            </p>
          </div>

          <div class="columns is-centered">
            <div class="column is-half">
              <img src="./static/images/SetupInteraction_page-0001.jpg" class="experimental-img" alt="Setup Interaction" />
            </div>
            <div class="column is-half">
              <img src="./static/images/SetupInteraction_2_page-0001.jpg" class="experimental-img" alt="Setup Interaction" />
            </div>
          </div>
          <br />
        </div>
      </div>
    </div>
  </section>


  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@data{IQ4AZY_2024,
        author = {Takele, Natnael Berhanu},
        publisher = {IIT Dataverse},
        title = {{MeGaBoT: A Meta Quest Dataset for Gaze, Body, and Hand Interaction in MR}},
        year = {2024},
        version = {DRAFT VERSION},
        doi = {10.48557/IQ4AZY},
        url = {https://doi.org/10.48557/IQ4AZY}
        }</code></pre>
      <!--  </div> -->
      <!--</section> -->


      <footer class="footer">
        <div class="container">
          <div class="content has-text-centered">
            <!-- <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a> -->
            <a class="icon-link" href="https://github.com/NatnaelB7" class="external-link">
              <i class="fab fa-github"></i>
            </a>
          </div>
          <div class="columns is-centered">
            <div class="column is-8">
              <div class="content">
                <p>
                  The template of this website was borrowed from <a
                    href="https://github.com/nerfies/nerfies.github.io">nerfies</a>. Check them out !
                </p>
                <p>
                  This website is licensed under a <a rel="license"
                    href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                    Commons Attribution-ShareAlike 4.0 International License</a>.
                </p>
              </div>
            </div>
          </div>
        </div>
      </footer>

</body>

</html>